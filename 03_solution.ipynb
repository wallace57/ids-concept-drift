{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 03: Adaptive Random Forest (ARF) – Giải pháp đề xuất\n",
    "\n",
    "Triển khai **ARF** (thư viện River) với **7 biến thể** drift detector để giảm Catastrophic Forgetting và thích nghi concept drift trên NSL-KDD, không lưu toàn bộ dữ liệu lịch sử. Đánh giá **theo phase** và báo cáo **AA, FM, BWT** (context §5–6).\n",
    "\n",
    "**Tham chiếu:** `exp_data/context/context_task_03.md`, `context_task_03_vie.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lưu ý:** Nếu chưa cài River, chạy lệnh sau trong terminal hoặc trong một cell: `pip install river`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Import\n",
    "\n",
    "Dùng **River** cho ARF (online learning, drift detection tích hợp). Cấu hình đường dẫn và hằng số theo STRICT config từ context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded. TARGET_CLASSES: ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup & Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# --- STRICT CONFIG (context_task_03) ---\n",
    "data_dir = Path(r'data/')\n",
    "train_file = data_dir / 'KDDTrain+.txt'\n",
    "test_file = data_dir / 'KDDTest+.txt'\n",
    "\n",
    "categorical_cols = ['protocol_type', 'service', 'flag']\n",
    "TARGET_CLASSES = ['Normal', 'DoS', 'Probe', 'R2L', 'U2R']\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "standard_column_names = [\n",
    "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent',\n",
    "    'hot', 'num_failed_logins', 'logged_in', 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
    "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate',\n",
    "    'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
    "    'attack_type', 'difficulty'\n",
    "]\n",
    "attack_categories = {\n",
    "    'Normal': 'normal',\n",
    "    'DoS': ['back', 'land', 'neptune', 'pod', 'smurf', 'teardrop', 'mailbomb', 'apache2', 'processtable', 'udpstorm'],\n",
    "    'Probe': ['ipsweep', 'nmap', 'portsweep', 'satan', 'mscan', 'saint'],\n",
    "    'R2L': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'phf', 'spy', 'warezclient', 'warezmaster', 'sendmail', 'named', 'snmpgetattack', 'snmpguess', 'xlock', 'xsnoop', 'worm'],\n",
    "    'U2R': ['buffer_overflow', 'loadmodule', 'perl', 'rootkit', 'httptunnel', 'ps', 'sqlattack', 'xterm']\n",
    "}\n",
    "print(\"Config loaded. TARGET_CLASSES:\", TARGET_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "Hàm load và map `attack_type` → `label` (5 lớp). Kiểm chứng: in `value_counts` và shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape: (125973, 44)\n",
      "test_df.shape: (22544, 44)\n",
      "\n",
      "Train label counts:\n",
      "label\n",
      "Normal    67343\n",
      "DoS       45927\n",
      "Probe     11656\n",
      "R2L         995\n",
      "U2R          52\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data Loading\n",
    "def load_and_process_data(file_path, cols, mapping_dict, drop_difficulty=False):\n",
    "    df = pd.read_csv(file_path, names=cols)\n",
    "    df['attack_type'] = df['attack_type'].astype(str).str.strip().str.rstrip('.')\n",
    "    reversed_mapping = {}\n",
    "    for group, value in mapping_dict.items():\n",
    "        if isinstance(value, list):\n",
    "            for sub in value:\n",
    "                reversed_mapping[sub] = group\n",
    "        else:\n",
    "            reversed_mapping[value] = group\n",
    "    df['label'] = df['attack_type'].map(reversed_mapping).fillna('Unknown')\n",
    "    if drop_difficulty:\n",
    "        df = df.drop(columns=['difficulty'], errors='ignore')\n",
    "    return df\n",
    "\n",
    "train_df = load_and_process_data(train_file, standard_column_names, attack_categories)\n",
    "test_df = load_and_process_data(test_file, standard_column_names, attack_categories)\n",
    "print(\"train_df.shape:\", train_df.shape)\n",
    "print(\"test_df.shape:\", test_df.shape)\n",
    "print(\"\\nTrain label counts:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering (Encoding)\n",
    "\n",
    "Cùng pipeline Task 2: loại bỏ `label`, `attack_type`, `difficulty`; OneHotEncoder cho categorical; remainder passthrough → 122 chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_encoded.shape: (125973, 122)\n",
      "X_test_encoded.shape: (22544, 122)\n",
      "y_train (first 3): ['Normal', 'Normal', 'DoS']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Encoding\n",
    "exclude_cols = ['label', 'attack_type', 'difficulty']\n",
    "X_train = train_df.drop(columns=exclude_cols)\n",
    "X_test = test_df.drop(columns=exclude_cols)\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('cat', encoder, categorical_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "preprocessor.fit(X_train)\n",
    "X_train_encoded = preprocessor.transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"X_train_encoded.shape:\", X_train_encoded.shape)\n",
    "print(\"X_test_encoded.shape:\", X_test_encoded.shape)\n",
    "print(\"y_train (first 3):\", y_train.head(3).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase Splitting\n",
    "\n",
    "Phase 0 = Normal, DoS, Probe; Phase 1 = R2L only; Phase 2 = U2R only. Feed stream theo thứ tự 0 → 1 → 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 0: 124926 samples\n",
      "Phase 1 (R2L): 995 samples\n",
      "Phase 2 (U2R): 52 samples\n",
      "Test set: 22544 samples\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Phase split\n",
    "mask_phase0 = y_train.isin(['Normal', 'DoS', 'Probe'])\n",
    "mask_phase1 = (y_train == 'R2L')\n",
    "mask_phase2 = (y_train == 'U2R')\n",
    "\n",
    "X_phase0 = X_train_encoded[mask_phase0.values]\n",
    "y_phase0 = y_train[mask_phase0].values\n",
    "X_phase1 = X_train_encoded[mask_phase1.values]\n",
    "y_phase1 = y_train[mask_phase1].values\n",
    "X_phase2 = X_train_encoded[mask_phase2.values]\n",
    "y_phase2 = y_train[mask_phase2].values\n",
    "\n",
    "print(\"Phase 0:\", X_phase0.shape[0], \"samples\")\n",
    "print(\"Phase 1 (R2L):\", X_phase1.shape[0], \"samples\")\n",
    "print(\"Phase 2 (U2R):\", X_phase2.shape[0], \"samples\")\n",
    "print(\"Test set:\", X_test_encoded.shape[0], \"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Helper: Convert sample to River dict\n",
    "\n",
    "River dùng `learn_one(x, y)` với `x` là dict. Chuyển mỗi hàng (122 số) thành dict `{\"f0\": v0, \"f1\": v1, ...}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys (first 5): ['f0', 'f1', 'f2', 'f3', 'f4']\n",
      "Len: 122\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: River input format\n",
    "def row_to_dict(row):\n",
    "    return {f\"f{i}\": float(x) for i, x in enumerate(row)}\n",
    "\n",
    "# Quick check\n",
    "sample = row_to_dict(X_phase0[0])\n",
    "print(\"Keys (first 5):\", list(sample.keys())[:5])\n",
    "print(\"Len:\", len(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define ARF variants (River)\n",
    "\n",
    "**Tham số chung:** `n_models=100`, `grace_period=50`. Bảy biến thể theo drift detector:\n",
    "- 1) Base: không drift detector.\n",
    "- 2) ADWIN: adaptive windowing, thay đổi trung bình (gradual drift).\n",
    "- 3) KSWIN: Kolmogorov–Smirnov, thay đổi phân phối.\n",
    "- 4) Page-Hinkley: thay đổi đột ngột trung bình (abrupt).\n",
    "- 5) DDM: dựa trên tỷ lệ lỗi.\n",
    "- 6) HDDM_A: Hoeffding A-test (moving average).\n",
    "- 7) HDDM_W: Hoeffding W-test (weighted moving average)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variants: ['ARF_Base', 'ARF_ADWIN', 'ARF_KSWIN', 'ARF_PH', 'ARF_DDM', 'ARF_HDDM_A', 'ARF_HDDM_W']\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build 7 ARF variants\n",
    "from river import forest\n",
    "from river import drift\n",
    "\n",
    "N_MODELS = 100\n",
    "GRACE_PERIOD = 50\n",
    "SEED = RANDOM_STATE\n",
    "\n",
    "def make_arf_base():\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=None, warning_detector=None)\n",
    "\n",
    "def make_arf_adwin():\n",
    "    d = drift.ADWIN(delta=0.001)\n",
    "    w = drift.ADWIN(delta=0.01)\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=w)\n",
    "\n",
    "def make_arf_kswin():\n",
    "    d = drift.KSWIN(alpha=0.005)\n",
    "    w = drift.KSWIN(alpha=0.01)\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=w)\n",
    "\n",
    "def make_arf_ph():\n",
    "    d = drift.PageHinkley()\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=None)\n",
    "\n",
    "def make_arf_ddm():\n",
    "    d = drift.DDM()\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=None)\n",
    "\n",
    "def make_arf_hddm_a():\n",
    "    d = drift.HDDM_A()\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=None)\n",
    "\n",
    "def make_arf_hddm_w():\n",
    "    d = drift.HDDM_W()\n",
    "    return forest.ARFClassifier(n_models=N_MODELS, grace_period=GRACE_PERIOD, seed=SEED, drift_detector=d, warning_detector=None)\n",
    "\n",
    "VARIANTS = [\n",
    "    ('ARF_Base', make_arf_base),\n",
    "    ('ARF_ADWIN', make_arf_adwin),\n",
    "    ('ARF_KSWIN', make_arf_kswin),\n",
    "    ('ARF_PH', make_arf_ph),\n",
    "    ('ARF_DDM', make_arf_ddm),\n",
    "    ('ARF_HDDM_A', make_arf_hddm_a),\n",
    "    ('ARF_HDDM_W', make_arf_hddm_w),\n",
    "]\n",
    "print(\"Variants:\", [v[0] for v in VARIANTS])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train stream & phase-wise evaluation (cho AA, FM, BWT)\n",
    "\n",
    "Hàm `train_arf_stream`: feed Phase 0 → 1 → 2 (không đánh giá giữa chừng). Hàm `train_arf_stream_with_eval`: feed từng phase rồi đánh giá ngay trên test set, trả về `(report_after_p0, report_after_p1, report_after_p2)` để tính AA, FM, BWT (context §5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_arf_stream and train_arf_stream_with_eval defined with logging.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Train stream (Phase 0 -> 1 -> 2) + phase-wise eval for AA/FM/BWT\n",
    "def log_progress(phase_name, idx, total, step=5000):\n",
    "    if total <= 0:\n",
    "        return\n",
    "    if (idx + 1) % step == 0 or idx == total - 1:\n",
    "        pct = (idx + 1) / total * 100\n",
    "        print(f\"{phase_name} progress: {idx + 1}/{total} ({pct:.1f}%)\", flush=True)\n",
    "\n",
    "def train_phase(model, phase_name, X, y):\n",
    "    total = len(X)\n",
    "    if total == 0:\n",
    "        print(f\"{phase_name}: no samples to train\", flush=True)\n",
    "        return\n",
    "    print(f\"{phase_name} training ({total} samples)...\", flush=True)\n",
    "    for idx, (x_row, y_val) in enumerate(zip(X, y)):\n",
    "        model.learn_one(row_to_dict(x_row), y_val)\n",
    "        log_progress(phase_name, idx, total)\n",
    "    print(f\"{phase_name} training complete\", flush=True)\n",
    "\n",
    "def train_arf_stream(model, X_phase0, y_phase0, X_phase1, y_phase1, X_phase2, y_phase2):\n",
    "    phases = [\n",
    "        (\"Phase 0\", X_phase0, y_phase0),\n",
    "        (\"Phase 1\", X_phase1, y_phase1),\n",
    "        (\"Phase 2\", X_phase2, y_phase2),\n",
    "    ]\n",
    "    for phase_name, X_batch, y_batch in phases:\n",
    "        train_phase(model, phase_name, X_batch, y_batch)\n",
    "    return model\n",
    "\n",
    "def evaluate_after_phase(model, phase_name, X_test_encoded, y_test):\n",
    "    print(f\"Evaluating after {phase_name}...\", flush=True)\n",
    "    report = classification_report(\n",
    "        y_test,\n",
    "        predict_arf_batch(model, X_test_encoded),\n",
    "        labels=TARGET_CLASSES,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(f\"Evaluation complete: {phase_name}\", flush=True)\n",
    "    return report\n",
    "\n",
    "def train_arf_stream_with_eval(model, X_phase0, y_phase0, X_phase1, y_phase1, X_phase2, y_phase2,\n",
    "                               X_test_encoded, y_test):\n",
    "    \"\"\"Train phase-by-phase; evaluate after each phase. Returns (report_p0, report_p1, report_p2).\"\"\"\n",
    "    reports = []\n",
    "    phases = [\n",
    "        (\"Phase 0\", X_phase0, y_phase0),\n",
    "        (\"Phase 1\", X_phase1, y_phase1),\n",
    "        (\"Phase 2\", X_phase2, y_phase2),\n",
    "    ]\n",
    "    for phase_name, X_batch, y_batch in phases:\n",
    "        train_phase(model, phase_name, X_batch, y_batch)\n",
    "        reports.append(evaluate_after_phase(model, phase_name, X_test_encoded, y_test))\n",
    "    return tuple(reports)\n",
    "\n",
    "print(\"train_arf_stream and train_arf_stream_with_eval defined with logging.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predict on test set (batch)\n",
    "\n",
    "Duyệt từng mẫu test, gọi `predict_one(x)` và thu thập dự đoán để dùng với `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict batch helper defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Predict batch\n",
    "def predict_arf_batch(model, X_test_encoded):\n",
    "    preds = []\n",
    "    for i in range(len(X_test_encoded)):\n",
    "        x = row_to_dict(X_test_encoded[i])\n",
    "        p = model.predict_one(x)\n",
    "        preds.append(p if p is not None else TARGET_CLASSES[0])\n",
    "    return preds\n",
    "\n",
    "print(\"Predict batch helper defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train & evaluate one variant (ARF_Base) – phase-wise\n",
    "\n",
    "Chạy thử ARF_Base với đánh giá sau từng phase (after_p0, after_p1, after_p2). In classification_report cuối để kiểm chứng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 9: ARF_Base & phase-wise evaluation ===\n",
      "Phase sizes -> Phase0: 124926, Phase1: 995, Phase2: 52\n",
      "Phase 0 training (124926 samples)...\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Train ARF_Base with phase-wise eval\n",
    "print('=== Step 9: ARF_Base & phase-wise evaluation ===')\n",
    "print(f'Phase sizes -> Phase0: {len(X_phase0)}, Phase1: {len(X_phase1)}, Phase2: {len(X_phase2)}')\n",
    "model_base = make_arf_base()\n",
    "rep0, rep1, rep2 = train_arf_stream_with_eval(\n",
    "    model_base, X_phase0, y_phase0, X_phase1, y_phase1, X_phase2, y_phase2,\n",
    "    X_test_encoded, y_test\n",
    ")\n",
    "print('Phase-wise evaluations done. Macro F1 summary:')\n",
    "print(f\"  Phase 0 macro F1: {rep0.get('macro avg', {}).get('f1-score', 0):.4f}\")\n",
    "print(f\"  Phase 1 macro F1: {rep1.get('macro avg', {}).get('f1-score', 0):.4f}\")\n",
    "print(f\"  Phase 2 macro F1 (final): {rep2.get('macro avg', {}).get('f1-score', 0):.4f}\")\n",
    "print('Executing final classification report on the test set...')\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    predict_arf_batch(model_base, X_test_encoded),\n",
    "    labels=TARGET_CLASSES,\n",
    "    zero_division=0\n",
    "))\n",
    "print('=== Step 9 completed ===')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train & evaluate all 7 variants (phase-wise)\n",
    "\n",
    "Với mỗi biến thể: train theo phase và lưu `after_p0`, `after_p1`, `after_p2` để sau đó tính AA, FM, BWT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Train all variants with phase-wise eval; store after_p0, after_p1, after_p2\n",
    "print('=== Step 10: Train all 7 ARF variants ===')\n",
    "results = {}\n",
    "total_variants = len(VARIANTS)\n",
    "for idx, (name, make_model) in enumerate(VARIANTS, 1):\n",
    "    print(f'--- [{idx}/{total_variants}] Training {name} ---')\n",
    "    model = make_model()\n",
    "    r0, r1, r2 = train_arf_stream_with_eval(\n",
    "        model, X_phase0, y_phase0, X_phase1, y_phase1, X_phase2, y_phase2,\n",
    "        X_test_encoded, y_test\n",
    "    )\n",
    "    macro_final = r2.get('macro avg', {}).get('f1-score', 0)\n",
    "    print(f'--- [{idx}/{total_variants}] {name} done. Final macro F1: {macro_final:.4f}')\n",
    "    results[name] = {\"after_p0\": r0, \"after_p1\": r1, \"after_p2\": r2}\n",
    "print('All variants finished.')\n",
    "print('Variants trained:', list(results.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary: F1 per class per variant\n",
    "\n",
    "In bảng F1 từng lớp cho từng biến thể (macro avg để so sánh nhanh)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: F1 summary table (final = after_p2)\n",
    "summary = []\n",
    "for name, reports in results.items():\n",
    "    report = reports[\"after_p2\"]\n",
    "    row = {\"variant\": name}\n",
    "    for cls in TARGET_CLASSES:\n",
    "        row[cls] = report.get(cls, {}).get(\"f1-score\", 0) or 0\n",
    "    row[\"macro_avg\"] = report.get(\"macro avg\", {}).get(\"f1-score\", 0) or 0\n",
    "    summary.append(row)\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. AA, FM, BWT – định nghĩa và helper (context §5)\n",
    "\n",
    "- **AA (Average Accuracy):** Trung bình hiệu năng trên từng task tại thời điểm cuối (sau P2). Task 0 = Normal/DoS/Probe (macro F1), Task 1 = R2L, Task 2 = U2R.\n",
    "- **FM (Forgetting Measure):** F1 tốt nhất của task (ngay sau khi học xong task) trừ F1 của task tại cuối. FM > 0 = quên.\n",
    "- **BWT (Backward Transfer):** F1 của task tại cuối trừ F1 ngay sau khi học task đó. BWT < 0 = ảnh hưởng tiêu cực (quên). Tham khảo Task 2 cho logic FM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Helpers for AA, FM, BWT\n",
    "CLASSES_TASK0 = [\"Normal\", \"DoS\", \"Probe\"]\n",
    "\n",
    "def f1_for_class(report, cls):\n",
    "    return report.get(cls, {}).get(\"f1-score\", 0) or 0\n",
    "\n",
    "def macro_f1_task0(report):\n",
    "    return np.mean([f1_for_class(report, c) for c in CLASSES_TASK0])\n",
    "\n",
    "def compute_aa_fm_bwt(reports):\n",
    "    \"\"\"reports = dict with keys after_p0, after_p1, after_p2 (each a classification_report dict).\"\"\"\n",
    "    r0, r1, r2 = reports[\"after_p0\"], reports[\"after_p1\"], reports[\"after_p2\"]\n",
    "    # AA: average over 3 tasks at final (after_p2)\n",
    "    aa = (macro_f1_task0(r2) + f1_for_class(r2, \"R2L\") + f1_for_class(r2, \"U2R\")) / 3.0\n",
    "    # FM: forgetting on task0 and task1 (task2 has no \"after\" to forget)\n",
    "    fm0 = macro_f1_task0(r0) - macro_f1_task0(r2)\n",
    "    fm1 = f1_for_class(r1, \"R2L\") - f1_for_class(r2, \"R2L\")\n",
    "    fm = (fm0 + fm1) / 2.0\n",
    "    # BWT: backward transfer (positive = good)\n",
    "    bwt0 = macro_f1_task0(r2) - macro_f1_task0(r0)\n",
    "    bwt1 = f1_for_class(r2, \"R2L\") - f1_for_class(r1, \"R2L\")\n",
    "    bwt = (bwt0 + bwt1) / 2.0\n",
    "    return aa, fm, bwt\n",
    "\n",
    "# Quick check on first variant\n",
    "name0 = list(results.keys())[0]\n",
    "aa0, fm0, bwt0 = compute_aa_fm_bwt(results[name0])\n",
    "print(f\"Example ({name0}): AA={aa0:.4f}, FM={fm0:.4f}, BWT={bwt0:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Bảng AA, FM, BWT theo từng biến thể\n",
    "\n",
    "In bảng so sánh AA, FM, BWT cho tất cả 7 biến thể (trước/sau khắc phục theo context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: AA, FM, BWT table per variant\n",
    "metrics_rows = []\n",
    "for name in results:\n",
    "    aa, fm, bwt = compute_aa_fm_bwt(results[name])\n",
    "    metrics_rows.append({\"variant\": name, \"AA\": aa, \"FM\": fm, \"BWT\": bwt})\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "print(\"=== AA, FM, BWT per variant (context §5) ===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(\"\\nFM > 0: forgetting; BWT < 0: negative backward transfer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
